{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab8 Assignment Task PROG8245 - NLP Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name: OKAFOR MICHAEL CHUKWUEMEKA\n",
    "### ID: 8953741"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the discussed topic in class for tokenizers, stop-word removal, stemming/lemmatization, and POS Tagging. <br><br>\n",
    "Create **ONE** function, that takes as an input a string, and returns the output of a string after stemming/lemmatization.<br><br>\n",
    "**Kindly note that you are required to consider the POS Tag while doing your stemming or lemmatization step (you should use whatever is more suitable for this task)** <br><br>\n",
    "After creating the function, you need to run your function on 10 **Random** files from reuters corpus, an example of how to download and load a file of reuters corpus is below. <br><br>\n",
    "**Your 10 **Random** files should be retrieved by getting a random array of length 10 which picks numbers RANDOMLY from 0 to len(reuters.fileids()), then the elements retrieved will be your corpus.<br> <br>*You need to set your Seed to be Equal to the last 3 digits in your studentID.*<br>** If your ID is 8000888 then seed =888 <br>\n",
    "**You may need to tailor your task based on the dataset to remove some special characters.**\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "def lemmatize_with_pos(sentence):\n",
    "    # Tokenize the input sentence into words\n",
    "    tokens = word_tokenize(sentence)\n",
    "    # Perform POS tagging\n",
    "    tags = pos_tag(tokens)\n",
    "    # Initialize lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Map POS tags to WordNet POS tags\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return 'a'  # ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return 'v'  # VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return 'n'  # NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return 'r'  # ADV\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Lemmatize each word with its corresponding POS tag\n",
    "    lemmatized_words = []\n",
    "    for word, tag in tags:\n",
    "        wordnet_pos = get_wordnet_pos(tag)\n",
    "        if wordnet_pos:\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "        else:\n",
    "            lemma = lemmatizer.lemmatize(word)\n",
    "        lemmatized_words.append(lemma)\n",
    "\n",
    "    return lemmatized_words\n",
    "\n",
    "def process_random_files():\n",
    "    # Set seed for reproducibility\n",
    "    random.seed(741)\n",
    "\n",
    "    # Get 10 random files from Reuters corpus\n",
    "    random_files = random.sample(reuters.fileids(), 10)\n",
    "\n",
    "    # Dictionary to store lemmatized content\n",
    "    lemmatized_files = {}\n",
    "\n",
    "    # Process each random file\n",
    "    for file_id in random_files:\n",
    "        # Read the file content\n",
    "        file_content = reuters.raw(file_id)\n",
    "        # Lemmatize the content\n",
    "        lemmatized_content = lemmatize_with_pos(file_content)\n",
    "        # Store lemmatized content in the dictionary\n",
    "        lemmatized_files[file_id] = lemmatized_content\n",
    "\n",
    "    return lemmatized_files\n",
    "\n",
    "# Run the function and get the results\n",
    "results = process_random_files()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last Step\n",
    "#### After finishing your code, run your code and save the result in a python dictionary, which would be of format:<br>\n",
    "#### {DocumentID: [List of Words], <br>\n",
    "#### ...} <br>\n",
    "#### Save your python dictionary as a JSON file, or Pickle file. <br>\n",
    "#### A sample code for saving a python dictionary is available at the end of this notebook.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized content saved as 'lemmatized_content.pickle'\n"
     ]
    }
   ],
   "source": [
    "# Save the dictionary as a pickle file\n",
    "with open(\"lemmatized_content.pickle\", \"wb\") as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "print(\"Lemmatized content saved as 'lemmatized_content.pickle'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ID: training/10925\n",
      "Lemmatized Words: ['AZTEC', 'MANUFACTURING', 'CO', '&', 'lt', ';', 'AZTC', '>', '4TH', 'QTR']\n",
      "-----------------------------------\n",
      "File ID: training/6636\n",
      "Lemmatized Words: ['ST.', 'JOSEPH', 'LIGHT', '&', 'lt', ';', 'SAJ', '>', 'SETS', 'SPLIT']\n",
      "-----------------------------------\n",
      "File ID: test/15457\n",
      "Lemmatized Words: ['UNIFIRST', 'CORP', '&', 'lt', ';', 'UNF', '>', 'SETS', 'QUARTERLY', 'Qtly']\n",
      "-----------------------------------\n",
      "File ID: training/3950\n",
      "Lemmatized Words: ['MEXICO', 'VEG', 'OIL', 'TAX', 'NOT', 'AIMED', 'AT', 'SUNFLOWER', '--', 'USDA']\n",
      "-----------------------------------\n",
      "File ID: training/4944\n",
      "Lemmatized Words: ['FED', 'SEEN', 'CONTENT', 'WITH', 'U.S.', 'FEBRUARY', 'ECONOMY', 'U.S.', 'February', 'report']\n",
      "-----------------------------------\n",
      "File ID: training/2122\n",
      "Lemmatized Words: ['CANADIAN', 'IMPERIAL', 'BANK', 'OF', 'COMMERCE', '1ST', 'QTR', 'SHARE', 'BASIC', '61']\n",
      "-----------------------------------\n",
      "File ID: test/17597\n",
      "Lemmatized Words: ['WALLACE', 'COMPUTER', '&', 'lt', ';', 'WCS', '>', 'BUY', 'OFFICE', 'PRODUCTS']\n",
      "-----------------------------------\n",
      "File ID: test/18621\n",
      "Lemmatized Words: ['OPEC', 'DIFFERENTIALS', 'NOT', 'SEEN', 'POSING', 'MAJOR', 'PROBLEMS', 'Some', 'OPEC', 'state']\n",
      "-----------------------------------\n",
      "File ID: training/10709\n",
      "Lemmatized Words: ['SINGAPORE', 'WELCOMES', 'NEW', 'LME', 'ALUMINIUM', 'CONTRACT', 'MOVE', 'Singapore', 'welcome', 'the']\n",
      "-----------------------------------\n",
      "File ID: training/12762\n",
      "Lemmatized Words: ['U.K', '.', 'WHEAT', 'MARKET', 'EASES', 'ON', 'INTERVENTION', 'RELEASE', 'U.K', '.']\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the pickle file\n",
    "with open(\"lemmatized_content.pickle\", \"rb\") as f:\n",
    "    lemmatized_contents = pickle.load(f)\n",
    "\n",
    "# Print the contents of the dictionary\n",
    "for file_id, words in lemmatized_contents.items():\n",
    "    print(\"File ID:\", file_id)\n",
    "    print(\"Lemmatized Words:\", words[:10])  # Print only the first 10 words\n",
    "    print(\"-----------------------------------\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving python dictionary:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
